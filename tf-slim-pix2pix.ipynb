{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import scipy.misc\n",
    "import matplotlib.pyplot as plt\n",
    "import tensorflow.contrib.slim as slim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def data_loader(file_path , batch_size, epochs):\n",
    "    filenames = tf.train.match_filenames_once(file_path)\n",
    "    filename_queue = tf.train.string_input_producer(filenames, num_epochs=epochs)\n",
    "    reader = tf.WholeFileReader()\n",
    "    _, img_bytes = reader.read(filename_queue)\n",
    "    image = tf.image.decode_jpeg(img_bytes, channels=3)\n",
    "    \n",
    "    return tf.train.batch([image], batch_size, dynamic_pad=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def lrelu(x, leaky=0.2):\n",
    "    return tf.maximum(x, leaky * x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def Unet_generator(cond_img):\n",
    "    with tf.variable_scope('generator'):\n",
    "        with slim.arg_scope([slim.conv2d],\n",
    "                            activation_fn=lrelu,\n",
    "                            kernel_size=[4,4],\n",
    "                            stride=2):\n",
    "            enc1 = slim.conv2d(cond_img , 64, scope='enc1')\n",
    "            enc2 = slim.batch_norm(slim.conv2d(enc1 , 128, scope='enc2'))\n",
    "            enc3 = slim.batch_norm(slim.conv2d(enc2 , 256, scope='enc3'))\n",
    "            enc4 = slim.batch_norm(slim.conv2d(enc3 , 512, scope='enc4'))\n",
    "            enc5 = slim.batch_norm(slim.conv2d(enc4 , 512, scope='enc5'))\n",
    "            enc6 = slim.batch_norm(slim.conv2d(enc5 , 512, scope='enc6'))\n",
    "            enc7 = slim.batch_norm(slim.conv2d(enc6 , 512, scope='enc7'))\n",
    "            enc8 = slim.batch_norm(slim.conv2d(enc7 , 512, kernel_size=[2,2], scope='enc8'))\n",
    "        \n",
    "        with slim.arg_scope([slim.conv2d_transpose],\n",
    "                            activation_fn = tf.nn.relu,\n",
    "                            kernel_size=[4,4],\n",
    "                            stride=2):\n",
    "            dec1 = slim.batch_norm(slim.conv2d_transpose(enc8 ,512, kernel_size=[2,2], scope='dec1'))\n",
    "            dec1 = tf.concat(3, [dec1, enc7] ,name='cat1')\n",
    "            dec2 = slim.batch_norm(slim.conv2d_transpose(dec1 ,512, scope='dec2'))\n",
    "            dec2 = tf.concat(3, [dec2, enc6],name='cat2')\n",
    "            dec3 = slim.batch_norm(slim.conv2d_transpose(dec2 ,512, scope='dec3'))\n",
    "            dec3 = tf.concat(3, [dec3, enc5],name='cat3')\n",
    "            dec4 = slim.batch_norm(slim.conv2d_transpose(dec3 ,512, scope='dec4'))\n",
    "            dec4 = tf.concat(3, [dec4, enc4],name='cat4')\n",
    "            dec5 = slim.batch_norm(slim.conv2d_transpose(dec4 ,256, scope='dec5'))\n",
    "            dec5 = tf.concat(3, [dec5, enc3],name='cat5')\n",
    "            dec6 = slim.batch_norm(slim.conv2d_transpose(dec5 ,128, scope='dec6'))\n",
    "            dec6 = tf.concat(3, [dec6, enc2],name='cat6')\n",
    "            dec7 = slim.batch_norm(slim.conv2d_transpose(dec6 ,64, scope='dec7'))\n",
    "            dec7 = tf.concat(3, [dec7, enc1],name='cat7')\n",
    "            dec8 = slim.conv2d_transpose(dec7 ,3, activation_fn=tf.nn.tanh)\n",
    "            \n",
    "            return dec8"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def discriminator(img,reuse=False):\n",
    "    with tf.variable_scope('discriminator',reuse=reuse):\n",
    "        with slim.arg_scope([slim.conv2d],\n",
    "                            activation_fn=lrelu,\n",
    "                            kernel_size=[4,4],\n",
    "                            stride=2):\n",
    "            x = slim.batch_norm(slim.conv2d(img , 64, scope='disc1'))\n",
    "            x = slim.batch_norm(slim.conv2d(x , 128, scope='disc2'))\n",
    "            x = slim.batch_norm(slim.conv2d(x , 256, scope='disc3'))\n",
    "            x = slim.batch_norm(slim.conv2d(x , 512, scope='disc4'))\n",
    "            \n",
    "            logits = slim.fully_connected(slim.flatten(x), 1, activation_fn=None ,scope='fc')\n",
    "            return tf.nn.sigmoid(logits) , logits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "train_file_path = './facades/train/*.jpg'\n",
    "batch_size = 10\n",
    "epochs = 200\n",
    "img_size = 256"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "batch_images = data_loader(train_file_path , batch_size , epochs)\n",
    "real_img , cond_img = batch_images[:,:,:img_size,:] , batch_images[:,:,img_size:,:]\n",
    "\n",
    "real_img.set_shape((batch_size , img_size, img_size, 3))\n",
    "cond_img.set_shape((batch_size , img_size, img_size, 3))\n",
    "\n",
    "real_img = tf.cast(real_img,tf.float32) / 127.5 -1\n",
    "cond_img = tf.cast(cond_img,tf.float32) / 127.5 -1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "fake_img = Unet_generator(cond_img)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "real_img_cond = tf.concat(3, [real_img , cond_img])\n",
    "fake_img_cond = tf.concat(3, [fake_img , cond_img])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "p_real,real_logits = discriminator(real_img_cond)\n",
    "p_fake,fake_logits = discriminator(fake_img_cond,reuse=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "d_loss_real = tf.reduce_mean(tf.nn.sigmoid_cross_entropy_with_logits(real_logits, tf.ones_like(p_real)))\n",
    "d_loss_fake = tf.reduce_mean(tf.nn.sigmoid_cross_entropy_with_logits(fake_logits, tf.zeros_like(p_fake)))\n",
    "\n",
    "d_loss = d_loss_real + d_loss_fake"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "g_loss = tf.reduce_mean(tf.nn.sigmoid_cross_entropy_with_logits(fake_logits, tf.ones_like(p_fake)))\n",
    "l1_loss = tf.reduce_mean(tf.abs(fake_img - real_img))\n",
    "g_loss = g_loss + 100 * l1_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "g_params = slim.get_variables(scope='generator')\n",
    "d_params = slim.get_variables(scope='discriminator')\n",
    "\n",
    "optimizer = tf.train.AdamOptimizer(0.0001)\n",
    "d_trainer = optimizer.minimize(d_loss, var_list=d_params)\n",
    "g_trainer = optimizer.minimize(g_loss, var_list=g_params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From <ipython-input-14-f670a3d9b746>:1 in <module>.: initialize_all_variables (from tensorflow.python.ops.variables) is deprecated and will be removed after 2017-03-02.\n",
      "Instructions for updating:\n",
      "Use `tf.global_variables_initializer` instead.\n",
      "WARNING:tensorflow:From <ipython-input-14-f670a3d9b746>:1 in <module>.: initialize_local_variables (from tensorflow.python.ops.variables) is deprecated and will be removed after 2017-03-02.\n",
      "Instructions for updating:\n",
      "Use `tf.local_variables_initializer` instead.\n"
     ]
    }
   ],
   "source": [
    "init_op = tf.group(tf.initialize_all_variables(), tf.initialize_local_variables())\n",
    "sess = tf.Session()\n",
    "sess.run(init_op)\n",
    "\n",
    "coord = tf.train.Coordinator()\n",
    "threads = tf.train.start_queue_runners(sess=sess,coord=coord)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:10 , d_loss:1.57773 ,g_loss:35.3845\n",
      "epoch:20 , d_loss:2.70714 ,g_loss:25.1553\n",
      "epoch:30 , d_loss:1.70816 ,g_loss:19.685\n",
      "epoch:40 , d_loss:1.84334 ,g_loss:15.8764\n",
      "epoch:50 , d_loss:2.63484 ,g_loss:14.797\n",
      "epoch:60 , d_loss:2.16098 ,g_loss:14.0988\n",
      "epoch:70 , d_loss:2.02101 ,g_loss:12.9068\n",
      "epoch:80 , d_loss:1.79013 ,g_loss:11.856\n",
      "epoch:90 , d_loss:2.31743 ,g_loss:12.6805\n",
      "epoch:100 , d_loss:2.46365 ,g_loss:10.4963\n",
      "epoch:110 , d_loss:1.87597 ,g_loss:10.2014\n",
      "epoch:120 , d_loss:2.15549 ,g_loss:9.29062\n",
      "epoch:130 , d_loss:1.66803 ,g_loss:9.18493\n",
      "epoch:140 , d_loss:2.12977 ,g_loss:9.40766\n",
      "epoch:150 , d_loss:3.00332 ,g_loss:8.20005\n",
      "epoch:160 , d_loss:2.21027 ,g_loss:8.85239\n",
      "epoch:170 , d_loss:1.98511 ,g_loss:8.40414\n",
      "epoch:180 , d_loss:2.71879 ,g_loss:7.79251\n",
      "epoch:190 , d_loss:2.40175 ,g_loss:7.24438\n",
      "Done training -- epoch limit reached\n"
     ]
    }
   ],
   "source": [
    "train_step = 1\n",
    "try:\n",
    "    while not coord.should_stop():\n",
    "        # Run training steps or whatever\n",
    "        _ , d_loss_, _, g_loss_  = sess.run([d_trainer , d_loss , g_trainer , g_loss])\n",
    "        #_ , g_loss_ = sess.run([g_trainer , g_loss])\n",
    "        \n",
    "        epcoh = (train_step * batch_size) / 400\n",
    "              \n",
    "        if (train_step * batch_size) % (400*10) == 0:\n",
    "            print \"epoch:%s , d_loss:%s ,g_loss:%s\" % (epcoh , d_loss_ ,g_loss_)\n",
    "        \n",
    "        if (train_step * batch_size) % (400*50) == 0: \n",
    "            real_img_ ,cond_img_ , fake_img_ = sess.run([real_img , cond_img , fake_img])\n",
    "            real_img_ = (real_img_ + 1) *127.5\n",
    "            cond_img_ = (cond_img_ + 1) *127.5\n",
    "            fake_img_ = (fake_img_ + 1) * 127.5\n",
    "            \n",
    "            for i in range(batch_size):\n",
    "                file_name = 'epcoh_%s_%s.jpg' % (epcoh , i)\n",
    "                canvas = np.zeros((img_size , img_size*3,3))\n",
    "                canvas[: , :img_size] = real_img_[i]\n",
    "                canvas[: , img_size:img_size*2] = cond_img_[i]\n",
    "                canvas[: , img_size*2:img_size*3] = fake_img_[i]                \n",
    "                scipy.misc.imsave(file_name, np.clip(canvas, 0, 255).astype('uint8'))\n",
    "                \n",
    "        train_step += 1\n",
    "        \n",
    "except tf.errors.OutOfRangeError:\n",
    "    print 'Done training -- epoch limit reached'\n",
    "    \n",
    "finally:\n",
    "    # When done, ask the threads to stop.\n",
    "    coord.request_stop()\n",
    "\n",
    "# Wait for threads to finish.\n",
    "coord.join(threads)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "variables_to_save = slim.get_variables(scope=\"generator\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "saver = tf.train.Saver(variables_to_save)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'./model/generator.ckpt'"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "saver.save(sess, \"./model/generator.ckpt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
